{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VOSK Speech Recognition\n",
    "In this notebook we will use the [vosk](https://alphacephei.com/vosk/) speech recognition toolkit to perform a speech recognition analysis on a video or audio file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "First, let's import all of our dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip cache purge > /dev/null 2>&1\n",
    "!pip install vosk > /dev/null 2>&1\n",
    "!pip install librosa > /dev/null 2>&1\n",
    "!pip uninstall -y gcu > /dev/null 2>&1\n",
    "!pip install -q git+https://github.com/jdchart/gcu.git > /dev/null 2>&1\n",
    "!apt install ffmpeg\n",
    "import vosk\n",
    "import gcu\n",
    "import os\n",
    "import wave\n",
    "import json\n",
    "import shutil\n",
    "import librosa\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import wiener"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's get the vosk model we want to use. The model we use will depend of the language of the content we wish to analyse - [here is a list](https://alphacephei.com/vosk/models) of models that vosk have avaiable. Change the `MODEL_NAME` variable to the model you wish to download and use.\n",
    "\n",
    "The `MODEL_PATH` variable allows you to choose where you would like to save the model. If the folder doesn't exist, it will get created. Even if you have already downloaded the model run this cell anyway as it will let the rest of the notebook know where to find the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these variables if needed:\n",
    "MODEL_NAME = \"vosk-model-small-en-us-0.15\"\n",
    "MODEL_PATH = os.path.join(os.path.abspath('../..'), \"models\")\n",
    "\n",
    "# Create folder if needed:\n",
    "if os.path.isdir(MODEL_PATH) == False:\n",
    "    os.makedirs(MODEL_PATH)\n",
    "\n",
    "# Download model if it doesn't already exist:\n",
    "if os.path.isdir(os.path.join(MODEL_PATH, MODEL_NAME)) == False:\n",
    "    jlu.files.download_zip(os.path.join(\"https://alphacephei.com/vosk/models\", MODEL_NAME) + \".zip\", MODEL_PATH)\n",
    "\n",
    "# Load the model:\n",
    "model = vosk.Model(os.path.join(MODEL_PATH, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get media\n",
    "Now we need to tell our code where to find the media we wish to process. Change the `SOURCE_FOLDER` variable to give the path to a folder of audio or video files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the path to a folder of audio or video files:\n",
    "SOURCE_FOLDER = os.path.join(os.path.abspath('../..'), \"sources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to make sure that the files are the right shape and format for vosk's model. For this, we shall use `ffmpeg` to convert everything to wav, to mono, and to a sample rate of 16000. We shall save the resulting audio in a temporary folder that will get created at `os.path.join(os.getcwd(), \"temp_src\")`.\n",
    "\n",
    "Note that only files with an extension that is in the `ACCEPTED_FORMATS` list will be treated. Feel free to add different formats if needed.\n",
    "\n",
    "Note that even if you have already converted the files, run this cell again as it will tell the rest of the notebook where to find the sources (without converting them again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCEPTED_FORMATS = [\"wav\", \"mp4\", \"mp3\"]\n",
    "\n",
    "# Create a temporary source folder:\n",
    "if os.path.isdir(os.path.join(os.getcwd(), \"temp_src\")) == False:\n",
    "    os.makedirs(os.path.join(os.getcwd(), \"temp_src\"))\n",
    "\n",
    "# Get a list of all the files to process:\n",
    "file_list = jlu.files.collect_files(SOURCE_FOLDER, ACCEPTED_FORMATS)\n",
    "source_list = []\n",
    "\n",
    "# Convert each file:\n",
    "for file in file_list:\n",
    "    print(f\"Converting \\\"{file}\\\"...\")\n",
    "    new_name = os.path.join(os.getcwd(), \"temp_src\", os.path.splitext(os.path.basename(file))[0] + '.wav')\n",
    "    if os.path.isfile(new_name) == False:\n",
    "        !ffmpeg -i \"{file}\" -ar {16000} -ac 1 \"{new_name}\" > /dev/null 2>&1\n",
    "    source_list.append(new_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Audio pre-processing\n",
    "Next we can manipulate the audio a bit so that it is easier for the vosk model to handle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in source_list:\n",
    "    print(f\"Processing {file}...\")\n",
    "    \n",
    "    # Load the audio file with librosa:\n",
    "    audio_data, sample_rate = librosa.load(file, sr = None)\n",
    "    \n",
    "    # Perform noise reduction and normalization:\n",
    "    noise_reduction = wiener(audio_data)\n",
    "    normalized = librosa.util.normalize(noise_reduction)\n",
    "    \n",
    "    # Scale to 16 bit depth for vosk:\n",
    "    scaled = np.int16(normalized * 32767)\n",
    "    \n",
    "    # Output file:\n",
    "    wavfile.write(file, sample_rate, scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Speech recognition\n",
    "Finally, we can run the speech recognition model on each audio file. First, let's just create a place to output our results by updating the `PROCESS_OUTPUT` variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give a path to a folder to output the results:\n",
    "PROCESS_OUTPUT = os.path.join(os.path.abspath('../..'), \"output\")\n",
    "\n",
    "# Create folder if needed:\n",
    "if os.path.isdir(PROCESS_OUTPUT) == False:\n",
    "    os.makedirs(PROCESS_OUTPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we run the recognition for each audio file, and save the results as a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in source_list:\n",
    "   print(f\"Processing {file}...\")\n",
    "\n",
    "   # Create the vosk recognizer:\n",
    "   recognizer = vosk.KaldiRecognizer(model, 16000)\n",
    "   recognizer.SetWords(True)\n",
    "\n",
    "   # Open the audio file:\n",
    "   with wave.open(os.path.join(\"media_for_analysis\", file), 'rb') as wf:\n",
    "      audio_data = wf.readframes(wf.getnframes())\n",
    "   \n",
    "   # Run the model:\n",
    "   result = recognizer.AcceptWaveform(audio_data)\n",
    "\n",
    "   # Export the results as a json file\n",
    "   if result:\n",
    "      res = json.loads(recognizer.Result())\n",
    "      result_file_name = os.path.join(PROCESS_OUTPUT, os.path.splitext(os.path.basename(file))[0] + \"_SPEECH_RECOGNITION.json\")\n",
    "      with open(result_file_name, 'w') as json_file:\n",
    "         json.dump(res, json_file, indent = 2)\n",
    "   else:\n",
    "      print(\"Speech recognition failed...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cleanup\n",
    "Finally, we can remove the temporary source folder we created now that we are finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(os.path.join(os.getcwd(), \"temp_src\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
